# -*- coding: utf-8 -*-
"""finetuning BERT.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1I82xUlHdMSWrfffvT12KkxP9zE4vExKV
"""

!pip install transformers datasets scikit-learn -q

import pandas as pd
from google.colab import drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/Thesis files/Training _files/sampled_EventRelevance_1x10_df.parquet'
df = pd.read_parquet(file_path)

# Map price to label
mapping = {1: 'shortage', -1: 'surplus', 0: 'stagnant'}
df['label'] = df['price'].map(mapping)

df

df['TEXT'] = df['TITLE'].astype(str) + ' ' + df['EVENT_TEXT'].astype(str)

from sklearn.preprocessing import LabelEncoder

label_encoder = LabelEncoder()
df['label_id'] = label_encoder.fit_transform(df['label'])

# Optional: View label mapping
label_mapping = dict(zip(label_encoder.classes_, label_encoder.transform(label_encoder.classes_)))
print(label_mapping)

from sklearn.model_selection import train_test_split

train_texts, val_texts, train_labels, val_labels = train_test_split(
    df['TITLE'].tolist(), df['label_id'].tolist(), test_size=0.2, random_state=42)

from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-uncased")

train_encodings = tokenizer(train_texts, truncation=True, padding=True, max_length=512)
val_encodings = tokenizer(val_texts, truncation=True, padding=True, max_length=512)

import torch

class CustomDataset(torch.utils.data.Dataset):
    def __init__(self, encodings, labels):
        self.encodings = encodings
        self.labels = labels

    def __getitem__(self, idx):
        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}
        item['labels'] = torch.tensor(self.labels[idx])
        return item

    def __len__(self):
        return len(self.labels)

train_dataset = CustomDataset(train_encodings, train_labels)
val_dataset = CustomDataset(val_encodings, val_labels)

from transformers import BertForSequenceClassification

model = BertForSequenceClassification.from_pretrained("bert-base-uncased", num_labels=3)

from transformers import Trainer, TrainingArguments

training_args = TrainingArguments(
    output_dir='./results',
    learning_rate=1e-4,
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    eval_strategy="epoch",
    save_strategy="epoch",
    logging_dir='./logs',
    logging_steps=10,
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
)

trainer.train()

trainer.evaluate()

import numpy as np

# Get raw predictions
predictions_output = trainer.predict(val_dataset)
logits = predictions_output.predictions
predicted_labels = np.argmax(logits, axis=1)

from sklearn.metrics import classification_report

# True labels
true_labels = val_labels

# Optional: decode labels back to string form
predicted_names = label_encoder.inverse_transform(predicted_labels)
true_names = label_encoder.inverse_transform(true_labels)

# Generate report
report = classification_report(true_names, predicted_names, digits=4)
print(report)

model.save_pretrained('Saved_BERT_52000')
tokenizer.save_pretrained('Saved_BERT_52000')

import shutil
shutil.make_archive('Saved_BERT_52000', 'zip', 'Saved_BERT_52000')

from google.colab import files
files.download('Saved_BERT_52000.zip')

